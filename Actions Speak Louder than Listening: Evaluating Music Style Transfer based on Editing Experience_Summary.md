## Title  
- Actions Speak Louder than Listening: Evaluating Music Style Transfer based on Editing Experience  
  
<br/>

## Problem Definition  
- 기존 음악 생성 연구의 경우 대부분 설문지 기반 청취 테스트를 통해 이루어짐  
- 사용자의 듣기 경험에 관련된 질문이며 음악가의 경험은 무시됨  
- 작곡가, 편곡자 및 음악 제작자의 관점에서 연구를 정량적으로 평가하는 경우는 거의 없음  

  
<br/>

## Motivation  
- 자기회귀 모델 및 트랜스포머를 결합한 새로운 음악 스타일 변화 모델을 설계하여 기존보다 개선된 모델(=BMST) 구축  
  
<br/>

## Method  
- 트랜스포머의 경우 시간 단계에 따라 동일한 음을 예측하는 경향이 존재하며, 이를 방지하기 위해 과거, 현재 및 미래의 세 부분으로 나눔  
  - 두 개의 개별 트랜스포머를 사용하여 과거 또는 미래의 정보만 독립적으로 모델링  
  - 순방향 트랜스포머는 과거 입력만 받고 역방향 트랜스포머는 현재 임베딩을 예측하기 위해 미래 입력만 받음  
- 트랜스포머의 **attention 메커니즘**은 순차적 데이터를 모델링 하는 데 적합   
- 트랜스포머의 경우 RNN에 비해 데이터의 순서를 인식하지 못한다는 단점 존재  
  - 최근에 해당 모델에 **상대 위치 임베딩**을 추가할 것을 제안하였으며 이로서 결과를 크게 개선함  
  
- 기본 모델의 CVAR 모듈에서는 특징을 추출할 때 음악의 절대 피치가 아닌 '상대 피치'를 학습하여 불안정한 결과 도출  
  - 절대 피치의 분포를 추가로 지정하고 훈련을 안정화하기 위해 피치 위치 인코딩 도입  
  - 7 옥타브의 피치 범위를 다루기 위해 7개의 확장 컨볼루션 레이어 사용  
  
<br/>

## Experiment setup  
- **Dataset**  
  - 음정의 수 = 84(A0 ~ G#7)
  - music21에서 추출한 바흐의 4부 합창 371곡
  - iReal Pro.1에서 수집한 재즈 미디엄 스윙 곡 487곡
  - 훈련 데이터 80%, 검증 10%, 테스트 10%
  
- **Model**  
  - 오디오 신호를 로그 스케일 크기 STFT 스펙트로그램으로 표현  
  - Adam 최적화 사용  
  - 32개의 에포크 동안 훈련하며 테슬라 V100 GPU에서 총약 20시간이 소요  
  
<br/>

## Result  
- **청취 테스트**   
  - 31명의 참가자를 대상  
  - 생성된 24개의 샘플들을 3개의 설문지에 할당  
  - 평가는 1(낮음)부터 5(높음)까지의 리커트 5점 척도  
  - "생성된 음악이 잘 들립니까?"라는 주관적인 질문에 답하도록 요청  
- **편곡 테스트**  
  - 상당한 음악적 배경을 가진 12명의 전문 음악 편곡자를 모집하여 BMST 모델에서 생성된 곡 편곡  
  - 음악 이론에 대한 지식에 따라 기존 음표를 이동, 분할, 길이 변경 또는 제거 등의 수정 요청  
  - 각 음악 작품에 대한 편곡은 30분 이내에 완료되어야 함  
  - 편곡 과정에서 편곡 시간, 키보드 누름 횟수, 마우스 클릭 횟수를 포함한 세 가지 메트릭을 기록  
  - 리커트 척도를 통한 설문 실시  
    - 해당 음악 작품이 편곡하기 쉬운지  
    - 편곡 후 음악 소리가 좋다고 생각하는지  
    - 모델에서 생성된 음악 클립이 음악 제작과정에 도움이 될수 있다고 생각하는지  
- 해당 결과 베이스라인 모델보다 결과가 향상됨   

<br/>

## Conclusion  
- 음악 스타일 변환 모델의 품질을 평가하는 데 마우스 클릭보다 키보드 누름 횟수, 편집 시간이 더 대표적임
- 청취 테스트와 편곡 테스트는 사용자의 경험의 다양한 측면을 나타내며 각 테스트는 서로를 대체할 수 없음
- 편곡 테스트의 경우 청취 테스트보다 더 많은 시간과 인력을 필요로 하며 비효율적이라는 문제 존재
- 하지만 청취보다는 편곡 과정에서의 행동들에 나타나는 정보가 풍부함  

<br/>

## Author Information  
- Wei-Tsung Lu, Meng-Hsuan Wu, Yuh-Ming Chiu, Li Su  

<br/>

## [Github Implementation](https://github.com/s603122001/Bidirectional-Music-Style-Transformer)  
